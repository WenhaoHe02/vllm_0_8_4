--- nixl.py
+++ nixl.py
@@
-import os
-import time
+import os
+import time
 import uuid
 from collections import defaultdict
 from typing import List, Tuple, Optional

 import msgspec
 import torch
 from vllm.config import VllmConfig
 from vllm.logger import init_logger

 from .kv_rearrange import rearrange_tensors
+from contextlib import contextmanager

 logger = init_logger(__name__)

@@
 class DynamoNixlConnector:
     def __init__(self, vllm_config: VllmConfig, engine_id: str, rank: int):
         self.vllm_config = vllm_config
@@
         self._downscale_info = {}
+
+        # ---- timing ----
+        self._timing = _Timing(
+            enabled=_env_flag("NIXL_TIMING", True),
+            tag=os.getenv("NIXL_TIMING_TAG", f"nixl.{engine_id}.r{rank}")
+        )
@@
     def _wait_many(self, handles):
+        with self._timing.span("wait_many"):
         # 轻量轮询 + 退避
         spins, SPIN_MAX = 0, 2000
         sleep_us, SLEEP_MAX = 50, 2000
         pending = list(handles)
         while pending:
@@
     def _chunk_iter(self, total_len: int, chunk: int):
         off = 0
         while off < total_len:
             end = min(off + chunk, total_len)
             yield off, end
             off = end
@@
     def _expand_blocks_to_tokens(self, block_ids: List[int]) -> List[int]:
+        with self._timing.span("expand_blocks_to_tokens"):
         B = int(self.block_size)
         out = []
         base_cache = {}
         for b in block_ids:
             base = base_cache.get(b)
@@
     def _write_blocks_down(self, local_block_ids, remote_block_ids, dst_engine_id, notify_msg):
+        with self._timing.span("write_down"):
         info = self._downscale_info[dst_engine_id]
         assert info is not None, "[WRITE-DOWN] downscale info missing"
@@
-        # 等非最后一批全部完成
+        # 等非最后一批全部完成
+        t_wait0 = time.perf_counter_ns()
         if inflight:
             self._wait_many(inflight)
             inflight.clear()
+        self._timing.add("write_down.wait_bulk_ns", time.perf_counter_ns() - t_wait0)

         # 组内 barrier：只有 leader 等全员到齐
-        self._barrier_mark_and_wait(dst_engine_id, notify_payload, info["group_size"], info["peer_idx"], is_leader)
+        t_bar0 = time.perf_counter_ns()
+        self._barrier_mark_and_wait(dst_engine_id, notify_payload, info["group_size"], info["peer_idx"], is_leader)
+        self._timing.add("write_down.barrier_ns", time.perf_counter_ns() - t_bar0)
@@
-        self.nixl_wrapper.transfer(h_last)
+        t_send_last0 = time.perf_counter_ns()
+        self.nixl_wrapper.transfer(h_last)
         self._wait_many([h_last])
+        self._timing.add("write_down.last_send_ns", time.perf_counter_ns() - t_send_last0)
         logger.info("[WRITE][DOWN] chunks=%d iov_per_req<=%d inflight<=%d", total_reqs + 1, MAX_IOV, MAX_INFLIGHT)

     def _read_blocks_down(self, local_block_ids, remote_block_ids, dst_engine_id):
+        with self._timing.span("read_down"):
         down = self._downscale_info[dst_engine_id]
         assert down is not None, "[READ-DOWN] downscale info missing"
@@
-        self.nixl_wrapper.transfer(h)
+        t_xfer0 = time.perf_counter_ns()
+        self.nixl_wrapper.transfer(h)
         while True:
             st = self.nixl_wrapper.check_xfer_state(h)
             if st == "DONE":
                 break
             if st != "PROC":
                 raise RuntimeError(f"[READ-DOWN] transfer failed: {st}")
             time.sleep(0.001)
+        self._timing.add("read_down.xfer_wait_ns", time.perf_counter_ns() - t_xfer0)
@@
     def _kv_block_u32sum(self, layer: int, entry_idx: int, block_id: int) -> int:
-
+        # 微观读性能影响极小，无需分段计时
         t = self.kv_caches[layer][entry_idx][block_id]  # shape: [block_size, num_heads_local, head_dim]
         return int(t.view(torch.int32).sum().item())
@@
     def read_blocks(self, local_block_ids, staging_block_ids, remote_block_ids, dst_engine_id):
+        with self._timing.span("read_blocks"):
         logger.info("[READ] local=%s staging=%s remote=%s dst_engine=%s",
                     len(local_block_ids), len(staging_block_ids), len(remote_block_ids), dst_engine_id)
@@
-        start_time = time.perf_counter()
+        start_time = time.perf_counter()
         if self._is_mla:
             staging_rearranging_ranges = None
             staging_block_ids = local_block_ids
@@
-        handles = []
+        handles = []
         t0 = time.perf_counter()
         for i in targets:
@@
-            self.nixl_wrapper.transfer(handle)
+            t_make0 = time.perf_counter_ns()
+            self.nixl_wrapper.transfer(handle)
+            self._timing.add("read_blocks.submit_ns", time.perf_counter_ns() - t_make0)
             handles.append(handle)
         logger.info("[READ] created_transfers=%s create_ms=%.3f",
                     len(handles), (time.perf_counter() - t0) * 1000.0)

         t1 = time.perf_counter()
         pending = list(handles)
+        wait_ns0 = time.perf_counter_ns()
         while pending:
             nxt = []
             for h in pending:
                 status = self.nixl_wrapper.check_xfer_state(h)
                 if status == "DONE":
                     continue
                 elif status == "PROC":
                     nxt.append(h)
                 else:
                     logger.error("[READ] transfer failed: state=%s", status)
                     raise RuntimeError(f"[READ] transfer failed with state {status}")
             pending = nxt
             if pending:
                 time.sleep(0.001)
+        self._timing.add("read_blocks.wait_ns", time.perf_counter_ns() - wait_ns0)
         logger.info("[READ] transfer_ms=%.3f", (time.perf_counter() - t1) * 1000.0)

         t2 = time.perf_counter()
         if not self._is_mla:
+            re_ns0 = time.perf_counter_ns()
             for local_range, staging_range in zip(local_rearranging_ranges, staging_rearranging_ranges):
                 logger.debug("[READ] rearrange cache_shape=%s local=%s staging=%s eff_tp=%s",
                              getattr(self.kv_caches[0], "shape", None), local_range, staging_range, eff_tp)
                 for kv_cache in self.kv_caches:
                     for cache in kv_cache:
                         rearrange_tensors(
                             cache[local_range[0]:local_range[1] + 1],
                             cache[staging_range[0]:staging_range[1] + 1],
                             eff_tp, "read"
                         )
+            self._timing.add("read_blocks.rearrange_ns", time.perf_counter_ns() - re_ns0)
         logger.info("[READ] rearrange_ms=%.3f total_ms=%.3f",
                     (time.perf_counter() - t2) * 1000.0,
                     (time.perf_counter() - start_time) * 1000.0)
@@
     def write_blocks(self, local_block_ids, staging_block_ids, remote_block_ids, dst_engine_id, notify_msg):
+        with self._timing.span("write_blocks"):
         try:
             logger.info("[WRITE] begin dst=%s local=%d staging=%d remote=%d notify_type=%s",
                         dst_engine_id, len(local_block_ids), len(staging_block_ids),
                         len(remote_block_ids), type(notify_msg).__name__)
@@
-            if down is not None:
+            if down is not None:
                 self._write_blocks_down(local_block_ids, remote_block_ids, dst_engine_id, notify_msg)
@@
-            if do_rearrange:
+            if do_rearrange:
                 t0 = time.perf_counter()
+                re_ns0 = time.perf_counter_ns()
                 for l_rng, s_rng in zip(_local_rearranging_ranges, staging_rearranging_ranges):
                     for kv_cache in self.kv_caches:
                         for cache in kv_cache:
                             rearrange_tensors(
                                 cache[l_rng[0]: l_rng[1] + 1],
                                 cache[s_rng[0]: s_rng[1] + 1],
                                 eff_tp, "write"
                             )
                 logger.info("[WRITE] rearrange_ms=%.3f", (time.perf_counter() - t0) * 1000)
+                self._timing.add("write_blocks.rearrange_ns", time.perf_counter_ns() - re_ns0)
@@
-                h = self.nixl_wrapper.make_prepped_xfer(
+                t_submit0 = time.perf_counter_ns()
+                h = self.nixl_wrapper.make_prepped_xfer(
                     "WRITE",
                     local_handle, staging_block_descs_ids,
                     remote_handle, remote_block_descs_ids,
                     notify_payload_str  # UP/EQ：允许 sideband 一起带
                 )
                 if notify_payload_str:
                     self._transfers.setdefault(notify_payload_str, []).append(h)
-                self.nixl_wrapper.transfer(h)
+                self.nixl_wrapper.transfer(h)
+                self._timing.add("write_blocks.submit_ns", time.perf_counter_ns() - t_submit0)
                 handles.append(h)
                 created += 1

             t1 = time.perf_counter()
             pending = list(handles)
+            wait_ns0 = time.perf_counter_ns()
             while pending:
                 nxt = []
                 for h in pending:
                     st = self.nixl_wrapper.check_xfer_state(h)
                     if st == "DONE":
                         continue
                     if st == "PROC":
                         nxt.append(h)
                     else:
                         logger.error("[WRITE] transfer failed state=%s", st)
                         raise RuntimeError(f"[WRITE] transfer failed with state {st}")
                 pending = nxt
                 if pending:
                     time.sleep(0.001)
+            self._timing.add("write_blocks.wait_ns", time.perf_counter_ns() - wait_ns0)
             logger.info("[WRITE] local_xfer_wait_ms=%.3f", (time.perf_counter() - t1) * 1000)
             logger.info("[WRITE] end ok dst=%s (UP/EQ)", dst_engine_id)
@@
     def add_remote_agent(
         self,
         engine_id: str,
         agent_metadata: List[bytes],
         agent_tp: int,
         kv_caches_base_addr: List[List[List[int]]],
         num_blocks: int,
         kv_caches_dev_ids: Optional[List[List[List[int]]]] = None,
     ):
+        with self._timing.span("add_remote_agent"):
         logger.info("[ADD] num_blocks=%d dev_ids=%s", num_blocks, "Y" if kv_caches_dev_ids is not None else "N")
@@
-            self.dst_xfer_side_handles[engine_id][0] = self.nixl_wrapper.prep_xfer_dlist(
+            t_prep0 = time.perf_counter_ns()
+            self.dst_xfer_side_handles[engine_id][0] = self.nixl_wrapper.prep_xfer_dlist(
                 self._remote_agents[engine_id][remote_rank], dst_desc
             )
+            self._timing.add("add_remote_agent.down_prep_dst_ns", time.perf_counter_ns() - t_prep0)
@@
-            descs = self.nixl_wrapper.get_xfer_descs(blocks_data, "VRAM")
-            self.src_xfer_side_handles[tp_multiplier] = self.nixl_wrapper.prep_xfer_dlist("", descs)
+            descs = self.nixl_wrapper.get_xfer_descs(blocks_data, "VRAM")
+            t_prep0 = time.perf_counter_ns()
+            self.src_xfer_side_handles[tp_multiplier] = self.nixl_wrapper.prep_xfer_dlist("", descs)
+            self._timing.add("add_remote_agent.up_prep_src_ns", time.perf_counter_ns() - t_prep0)
@@
-            descs = self.nixl_wrapper.get_xfer_descs(blocks_data, "VRAM")
-            self.dst_xfer_side_handles[engine_id][i] = self.nixl_wrapper.prep_xfer_dlist(
+            descs = self.nixl_wrapper.get_xfer_descs(blocks_data, "VRAM")
+            t_prep1 = time.perf_counter_ns()
+            self.dst_xfer_side_handles[engine_id][i] = self.nixl_wrapper.prep_xfer_dlist(
                 self._remote_agents[engine_id][remote_rank], descs
             )
+            self._timing.add("add_remote_agent.up_prep_dst_ns", time.perf_counter_ns() - t_prep1)
@@
     def get_done_tranfers(self) -> List[str]:
+        with self._timing.span("get_done_transfers"):
         done_req_ids: List[str] = []
@@
         return done_req_ids
+
+    # -------- public timing API --------
+    def get_timing(self, reset: bool = False):
+        """返回当前累计的计时统计（单位 ns / ms / 平均值），可选清零。"""
+        stats = self._timing.snapshot(reset=reset)
+        if stats:
+            logger.debug("[TIMING] %s", stats)
+        return stats
+
+
+# ===== helpers & timing class =====
+def _env_flag(name: str, default: bool) -> bool:
+    v = os.getenv(name)
+    if v is None:
+        return default
+    return v.strip() not in ("0", "false", "False", "")
+
+
+class _Timing:
+    """轻量计时聚合：with timing.span('key'): ... / timing.add('key_ns', ns)"""
+    __slots__ = ("_enabled", "_ns", "_n", "_tag")
+
+    def __init__(self, enabled: bool, tag: str = "nixl"):
+        self._enabled = bool(enabled)
+        self._ns = defaultdict(int)   # key -> total ns
+        self._n = defaultdict(int)    # key -> calls
+        self._tag = tag
+
+    @contextmanager
+    def span(self, key: str):
+        if not self._enabled:
+            yield
+            return
+        t0 = time.perf_counter_ns()
+        try:
+            yield
+        finally:
+            dt = time.perf_counter_ns() - t0
+            self._ns[key] += dt
+            self._n[key] += 1
+
+    def add(self, key: str, ns: int):
+        if not self._enabled:
+            return
+        self._ns[key] += int(ns)
+        self._n[key] += 1
+
+    def snapshot(self, reset: bool = False):
+        if not self._enabled:
+            return {}
+        out = {}
+        for k, tot in self._ns.items():
+            n = max(1, self._n.get(k, 1))
+            out[f"{self._tag}.{k}.ns"] = int(tot)
+            out[f"{self._tag}.{k}.ms"] = round(tot / 1e6, 3)
+            out[f"{self._tag}.{k}.avg_ms"] = round((tot / n) / 1e6, 3)
+            out[f"{self._tag}.{k}.calls"] = int(self._n.get(k, 0))
+        if reset:
+            self._ns.clear()
+            self._n.clear()
+        return out
